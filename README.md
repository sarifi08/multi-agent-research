# Multi-Agent Research System

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-412991.svg)](https://openai.com/)
[![Tavily](https://img.shields.io/badge/Tavily-Web%20Search-orange.svg)](https://tavily.com/)

> A multi-agent AI system that breaks down complex research queries into parallel sub-searches, filters results by relevance, and produces a polished research report â€” all orchestrated through a shared-state architecture with full cost & timing monitoring.

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Planner â”‚ â”€â”€â–¶ â”‚ Researchers  â”‚ â”€â”€â–¶ â”‚ Analyst  â”‚ â”€â”€â–¶ â”‚ Writer â”‚
â”‚         â”‚     â”‚ (parallel)   â”‚     â”‚          â”‚     â”‚        â”‚
â”‚ Breaks  â”‚     â”‚ Async web    â”‚     â”‚ Filters  â”‚     â”‚ Writes â”‚
â”‚ query   â”‚     â”‚ search +     â”‚     â”‚ & ranks  â”‚     â”‚ report â”‚
â”‚ into    â”‚     â”‚ retry logic  â”‚     â”‚ results  â”‚     â”‚ with   â”‚
â”‚ sub-    â”‚     â”‚              â”‚     â”‚          â”‚     â”‚ stream â”‚
â”‚ queries â”‚     â”‚              â”‚     â”‚          â”‚     â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                                          â–¼
                                                   Research Report
```

## âœ¨ Features

- **Multi-agent orchestration** â€” four specialized AI agents (Planner â†’ Researcher â†’ Analyst â†’ Writer) coordinated through a shared-state "whiteboard" pattern
- **True async parallelism** â€” researchers run genuinely concurrent web searches using `aiohttp`, not thread-pool workarounds
- **Smart retry logic** â€” failed searches are automatically rephrased by the Planner and retried, with configurable retry limits
- **Cost & performance monitoring** â€” every agent's token usage, cost, and timing is tracked and reported in a detailed breakdown
- **Streaming output** â€” the Writer streams the final report token-by-token for a responsive CLI experience
- **Interactive CLI** â€” full `argparse` interface with `--query`, `--model`, `--export`, and `--no-stream` flags
- **Streamlit Web UI** â€” visual interface to run research, watch agents work, and download reports
- **Search caching** â€” disk-backed cache with TTL expiry avoids burning API credits on repeated queries
- **Markdown export** â€” save reports as `.md` files from CLI or download from the web UI
- **Clean separation of concerns** â€” session (data), tracker (observability), and agents (logic) are fully decoupled

## ğŸ“ Project Structure

```
multi-agent-research/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ planner.py          # Breaks user query into sub-searches
â”‚   â”œâ”€â”€ researcher.py       # Async web search with retry logic
â”‚   â”œâ”€â”€ analyst.py          # Filters & ranks results by relevance
â”‚   â””â”€â”€ writer.py           # Composes final report (streaming)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ orchestrator.py     # Coordinates the 4-agent pipeline
â”‚   â””â”€â”€ session.py          # Shared state (the "whiteboard")
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ web_search.py       # Async Tavily search wrapper (aiohttp)
â”‚   â””â”€â”€ cache.py            # Disk-backed search cache with TTL
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py         # Pydantic settings from .env
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ tracker.py          # Cost & timing tracker per agent
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_planner.py     # Unit tests with mocked OpenAI
â”‚   â”œâ”€â”€ test_researcher.py  # Async tests with mocked search
â”‚   â”œâ”€â”€ test_analyst.py     # Filter & ranking tests
â”‚   â”œâ”€â”€ test_writer.py      # Report generation tests
â”‚   â”œâ”€â”€ test_orchestrator.py # Pipeline integration tests
â”‚   â””â”€â”€ test_cache.py       # Cache TTL + persistence tests
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/tests.yml # CI: pytest on Python 3.10-3.12
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/     # Bug report & feature request templates
â”‚   â””â”€â”€ pull_request_template.md
â”œâ”€â”€ app.py                  # Streamlit web UI
â”œâ”€â”€ example.py              # CLI entry point
â”œâ”€â”€ pyproject.toml          # Modern Python packaging
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ—ï¸ Architecture

### Shared-State "Whiteboard" Pattern

All agents read from and write to a single `ResearchSession` object instead of passing data like a relay race. This means every agent has full context at all times:

```python
@dataclass
class ResearchSession:
    query: str                          # User's original question
    sub_queries: List[str]              # Planner output
    raw_results: List[dict]             # Researcher output
    findings: List[dict]                # Analyst output
    report: str                         # Writer output
    agent_statuses: dict                # Who's doing what
    logs: List[str]                     # Full audit trail
```

### Agent Pipeline

| Step | Agent | Input | Output | LLM Calls |
|------|-------|-------|--------|-----------|
| 1 | **Planner** | User query | 3â€“4 targeted sub-queries | 1 |
| 2 | **Researchers** | Sub-queries (parallel) | Raw search results per query | 0â€“2 per retry |
| 3 | **Analyst** | All raw results | Filtered & ranked findings | 1 per result |
| 4 | **Writer** | Approved findings | Polished research report | 1 (streamed) |

### Monitoring

Every agent run is tracked with:
- â± **Timing** â€” start/end timestamps and duration
- ğŸª™ **Token usage** â€” input and output tokens per agent
- ğŸ’° **Cost** â€” calculated from model pricing (GPT-4o, GPT-4-turbo, GPT-3.5-turbo)
- ğŸ“‹ **Audit log** â€” timestamped log of every action in the pipeline

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- [OpenAI API key](https://platform.openai.com/api-keys)
- [Tavily API key](https://tavily.com/) (free tier available)

### Installation

```bash
# Clone the repository
git clone https://github.com/sarifi08/multi-agent-research.git
cd multi-agent-research

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # macOS/Linux
# .venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env and add your API keys
```

### Configuration

Create a `.env` file (or copy from `.env.example`):

```env
OPENAI_API_KEY=sk-your-openai-key-here
TAVILY_API_KEY=tvly-your-tavily-key-here
```

Optional settings in `.env`:

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_MODEL` | `gpt-4o` | OpenAI model (`gpt-4o`, `gpt-4-turbo`, `gpt-3.5-turbo`) |
| `MAX_OUTPUT_TOKENS` | `2000` | Max tokens for Writer output |
| `MAX_RETRIES` | `2` | Researcher retry attempts before giving up |
| `MAX_PARALLEL_SEARCHES` | `3` | Concurrent search limit (rate limit protection) |
| `MAX_SEARCH_RESULTS` | `5` | Results per search query |
| `ENABLE_TRACKING` | `true` | Cost & timing monitoring |

### Usage

#### CLI (Interactive)

```bash
# Interactive prompt â€” just run it
python example.py

# Direct query
python example.py "What are the latest breakthroughs in AI agents?"

# With options
python example.py --query "AI in healthcare" --model gpt-3.5-turbo
python example.py "quantum computing 2024" --no-stream --export report.md
```

**CLI Options:**

| Flag | Description |
|------|-------------|
| `query` | Research query (positional or `--query`) |
| `--model`, `-m` | Override LLM model (default: from `.env`) |
| `--no-stream` | Get full report at once (no streaming) |
| `--export`, `-e` | Export report to Markdown file |

#### Web UI (Streamlit)

```bash
streamlit run app.py
```

The web UI provides:
- ğŸ” Query input with real-time agent status
- ğŸ“Š Cost, timing, and source metrics
- ğŸ“¥ One-click Markdown report download
- ğŸ“œ Search history in sidebar

**Example CLI output:**

```
ğŸ” Query: What are the latest breakthroughs in AI agents in 2024?

[1/4] ğŸ§  Planner running...
[2/4] ğŸ” Researchers running (4 parallel)...
[3/4] ğŸ“Š Analyst running...
[4/4] âœï¸  Writer running...

â”€â”€ RESEARCH REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[Streamed report appears here token by token...]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š RESEARCH SESSION SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Query:    What are the latest breakthroughs in AI agents in 2024?
Success:  âœ…
Duration: 32.1s
Cost:     $0.0342
Sources:  8

â”€â”€ Agent Breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent               Time       Cost     Tokens
--------------------------------------------------
planner             2.1s $   0.0012        280
researcher          8.4s $   0.0000          0
analyst            12.3s $   0.0180      3,200
writer              9.3s $   0.0150      2,800
```

## ğŸ§ª Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run a specific test file
pytest tests/test_planner.py -v

# Run with coverage report
pytest tests/ -v --cov=agents --cov=core --cov=tools --cov=monitoring --cov-report=term-missing
```

**Test coverage includes:**

| Module | Test File | What's Tested |
|--------|-----------|---------------|
| Planner | `test_planner.py` | Query decomposition, bad LLM output handling |
| Researcher | `test_researcher.py` | Async search, retry logic, failure recovery |
| Analyst | `test_analyst.py` | Score filtering, relevance judgment, empty results |
| Writer | `test_writer.py` | Report generation, streaming, empty findings |
| Orchestrator | `test_orchestrator.py` | Full pipeline, model override, failure handling |
| Cache | `test_cache.py` | Hit/miss, TTL expiry, corruption recovery, persistence |

All tests use `unittest.mock` â€” **no API keys needed** to run tests.

## ğŸ”§ How It Works (Detailed)

### 1. Planner Agent
Takes the user's broad research question and breaks it into 3â€“4 specific, non-overlapping search queries. Uses a low temperature (0.3) for consistency.

### 2. Researcher Agents (Parallel)
Each sub-query spawns an async researcher that:
- Searches the web via Tavily API using `aiohttp` (true async, not threaded)
- Checks if results are useful (average relevance â‰¥ 0.5)
- If poor results: asks the Planner LLM to rephrase and retries
- Respects `MAX_PARALLEL_SEARCHES` to avoid API rate limits

### 3. Analyst Agent
Reviews all raw results against the **original** user query (catches topic drift):
- Pre-filters by Tavily's relevance score (â‰¥ 0.5, falls back to â‰¥ 0.3)
- LLM judges each result's relevance with reasoning
- Sorts findings by relevance score

### 4. Writer Agent
Composes the final report from approved findings:
- Writes in natural prose organized by themes (not bullet dumps)
- Cites sources inline
- Supports token-by-token streaming for responsive output
- Temperature 0.7 for creative but grounded writing

## ğŸ“Š Cost Estimates

| Model | Typical Query Cost | Speed |
|-------|-------------------|-------|
| `gpt-4o` | ~$0.03â€“0.05 | ~30s |
| `gpt-4-turbo` | ~$0.05â€“0.08 | ~40s |
| `gpt-3.5-turbo` | ~$0.002â€“0.005 | ~20s |

## ğŸ› ï¸ Tech Stack

- **[OpenAI API](https://openai.com/)** â€” powers all four agents (planning, rephrasing, analysis, writing)
- **[Tavily API](https://tavily.com/)** â€” advanced web search with relevance scoring
- **[aiohttp](https://docs.aiohttp.org/)** â€” true async HTTP for parallel searches
- **[Streamlit](https://streamlit.io/)** â€” web UI for visual research interface
- **[Pydantic Settings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/)** â€” type-safe configuration from `.env`
- **[Loguru](https://github.com/Delgan/loguru)** â€” structured logging with emoji
- **[pytest](https://docs.pytest.org/)** â€” unit testing with async support and coverage

## ğŸ¤ Contributing

Contributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.
